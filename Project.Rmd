---
title: 'PML Project: Human Activity Recognition'
author: "Ray Jones"
date: "22nd November, 2015"
output:
  html_document:
    keep_md: yes
  pdf_document:
    highlight: default
    toc: no
---
```{r, echo=FALSE}
library(knitr)
knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption">',options$htmlcap,"</p>",sep="")
    }
    })
```

### Executive Summary
This document has been prepared as a submission in response to the Project requirements for the Practical Machine Learning (PML) module of the Coursera/Johns Hopkins Data Science Specialization. The aim of the project was to train a machine learning (ML) algorithm to analyse a data set consisting of quantified self movement measurements collected from accelerometers worn by a number of people during exercises. After initialization and pre-processing of the data, a number of different ML algorithms were trained and their in-sample performances assessed. On this basis, the best ML algorithm was selected and subjected to out-of-sample performance assessment. The selected ML algorithm was then used to generate a prediction of how well the exercises were performed for $20$ blind test cases. This prediction was submitted for grading and a $100 \%$ accuracy level was achieved using the randomForest ML algorithm.

### Exploratory Analysis & Data Pre-processing
The data sets for this project are available from https://d396qusza40orc.cloudfront.net/predmachlearn/ - they consist of pml-training.csv which includes the training/testing data and pml-testing.csv which includes the 20 blind test cases for the project submission. Full instructions on this project are available at https://class.coursera.org/predmachlearn-034/human_grading/view/courses/975204/assessments/4/submissions - the collection and original analysis of this data set are described in a paper[^1] available online from http://groupware.les.inf.puc-rio.br/har .

[^1]: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

The code chunk below loads the various required libraries and then sets the random number seed in order to make the processing reproducible. It then sets the working directory and loads the two data files required.

```{r initialise, cache=TRUE}
## Install the required libraries
library(caret, quietly=T);library(parallel, quietly=T);library(doParallel, quietly=T);library(knitr, quietly=T)
library(caTools, quietly=T);library(rpart, quietly=T);library(ipred, quietly=T);library(plyr, quietly=T)
library(survival, quietly=T);library(splines, quietly=T);library(MASS, quietly=T); library(kernlab, quietly=T)
library(gbm, quietly=T);library(randomForest, quietly=T)

## Set the random number seed to make this all reproducible
set.seed(1221)

## Set the working directory and then load the required data files
dirPath             <- "~/Documents/Data Science Specialisation/8 - Practical Machine Learning/Project"
setwd(dirPath)
dataSet 			<- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
submissionCases 	<- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
```

The next code chunk prints out the dimensions of the two data sets - (i) dataSet, which contains the data for the training and testing of the ML algorithms and (ii) submissionCases, which contains the $20$ individual observations used for the blind prediction performance testing submitted for grading. This chunk also examined the structures of the two data sets but these lines were subsequently removed as the outputs were very verbose. In the original analysis the data were treated as time series. However, this was not an option for this exercise as the $20$ blind test cases were supplied out of context (ie without histories or future samples) so the analysis reported here was done on the basis of the instantaneous measurement values and not via time-slicing. 

```{r preprocess1, cache=TRUE}
## Exploratory data analysis
output 				<- rbind(dim(submissionCases), dim(dataSet))
colnames(output) 	<- c("Observations", "Variables")
rownames(output) 	<- c("Submission Cases", "Training/testing Data")
kable(output)
```

The next step was to remove the first seven variables from each data set, as they contain values added to the raw measurement data during the original analysis which were not relevant to the analysis here. Following this, variables with incomplete (missing) measurements were identified and removed (as these were also added during the original analysis, missing samples generally being a consequence of the time windowing imposed during that analysis). In order to reduce the computational load involved in ML algorithm training, pairs of strongly correlated variables were next identified using the findCorrelations() function and one of each pair was removed from the data set (since one of each pair was redundant as a predictor due to the correlation). The dimensions of the resulting data sets were then printed. 

```{r preprocess2, cache=TRUE}
## Remove irrelevant and empty variables
dataSet 			<- dataSet[,-c(1:7)]
submissionCases 	<- submissionCases[,-c(1:7)]
dataSet 			<- dataSet[,colSums(is.na(dataSet)) == 0]
submissionCases 	<- submissionCases[,colSums(is.na(submissionCases)) == 0]

## Find strongly correlated variables & remove from data sets
correlations 		<- findCorrelation(cor(dataSet[,1:52]), cutoff=0.8)
dataSet 			<- dataSet[,-c(correlations)]
submissionCases 	<- submissionCases[,-c(correlations)]
output 				<- rbind(dim(submissionCases), dim(dataSet))
colnames(output) 	<- c("Observations", "Variables")
rownames(output) 	<- c("Submission Cases", "Training/testing Data")
kable(output)
```

As a final check before data partitioning, an assessment was carried out to establish that no zero or near zero variance predictors were present in the final filtered data sets (since predictors that don't change value over the data set are not likely to be useful as predictors) - this was confirmed. 

```{r preprocess3, cache=TRUE}
## Confirm absence of zero & near zero variance terms
dataSet_nzv 		<- nearZeroVar(dataSet, saveMetrics = TRUE)
submissionCases_nzv <- nearZeroVar(submissionCases, saveMetrics = TRUE)
output 				<- rbind(cbind(sum(as.numeric(submissionCases_nzv$zeroVar)), 
						  		   sum(as.numeric(submissionCases_nzv$nzv))), 
							 cbind(sum(as.numeric(dataSet_nzv$zeroVar)), 
							 	   sum(as.numeric(dataSet_nzv$nzv))))
colnames(output) 	<- c("Zero Variance", "Near Zero Variance")
rownames(output) 	<- c("Submission Cases", "Training/testing Data")
kable(output)
```

Finally, the code chunk below was used to randomly partition dataSet in to training ($60 \%$) and testing ($40 \%$) sub-sets (to allow training, in-sample and out-of-sample testing) and then print the dimensions of each. 

```{r preprocess4, cache=TRUE}
## Partition the dataSet into 60% training and 40% test data sets
intrain 			<- createDataPartition(dataSet$classe, p = 0.60, list = FALSE)
training 			<- dataSet[intrain,]
testing 			<- dataSet[-intrain,]
output 				<- rbind(dim(testing), dim(training))
colnames(output) 	<- c("Observations", "Variables")
rownames(output) 	<- c("Testing data", "Training data")
kable(output)
```

As can be seen above, after pre-processing the data sets consist of $40$ variables. In the training & testing sets this includes the $classe variable which contains the truth for the quality of exercise performance. In submissionCase the $classe variable is absent, replaced by a test id no. (ie for the submissionCases the actual truth data is unknown a-priori and hence these test cases are blind).  

### Model Training
As the next step, the code chunk below was used to train a total of eight different methods interfaced from within the R {caret} package using the training sub-set of the partitioned data set. The {caret}train() method automatically included an element of cross-validation against suitably sub-setted training data as part of the training process. But it should be noted that this is quite distinct from the explicit testing manually carried out and documented below (against the testing sub-set of the partitioned data), which supported the a-priori estimation of the out-of-sample performance accuracy for the blind submission test cases (recall that error = 1 - accuracy).

The ML algorithms trained were selected to cover a range of different approaches and so maximise the probability of discovering an optimal approach. This process was initially found to be extremely computationally demanding, eg, the initial runs of this script required in excess of 4 hours to complete. As part of the optimisation of the code a number of strategies were investigated. The following optimisations were finally implemented after significant experimentation:

* For all MLs, the default {caret}train() bootstrap re-sampling process was replaced by a 3-fold cross-validation which had no impact on accuracy but significantly speeded up the execution (using {caret}trainControl() parameters "method" & "number").  
* For the random forest ML algorithm, the default selection of no of predictors/3 for the no. of predictors randomly selected at each node splitting step of the tree was replaced by 6 (parameter "mtry").
* The multi-core nature of the machine used to execute the script was exploited by turning on parallel processing using the {doParallel}registerDoParallel() routine. This was found to give a significant speed boost during execution.

The steps outlined above were very successful and reduced the elapsed time required for executing this script from > 4 hours to < 10 minutes.

The eight ML algorithms selected for training and later cross comparison of in-sample results were:  

* Random Forest (rf)  
* Recursive Partioning & Regression Trees (rpart)  
* Bagged Trees (treebag)  
* Boosted Trees (gbm)  
* Linear Discriminant Analysis (lda)  
* Support Vector Machines - Linear (svmLinear)  
* Support Vector Machines - Radial (svmRadial)  
* Logit Boost Classification (LogitBoost)  

For further discussion of the details of any of these ML algorithms the reader is referred to the relevant R documentation (typing ?method_name at the R command prompt - after the relevant libraries have been loaded).

```{r train_models, cache=TRUE}
## Set up parallel processing
cluster 			<- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

## Train candidate models
fitControl 			<- trainControl(method = "cv", number = 3, verboseIter = FALSE)
tgrid 				<- expand.grid(mtry=c(6)) 
model_rf 			<- train(classe ~ ., data=training, method="rf", trControl=fitControl, tuneGrid=tgrid)
model_rpart			<- train(classe ~ ., data=training, method="rpart", trControl=fitControl)
model_treebag		<- train(classe ~ ., data=training, method="treebag", trControl=fitControl)
model_gbm 			<- train(classe ~ ., data=training, method="gbm", trControl=fitControl, verbose=FALSE)
model_lda			<- train(classe ~ ., data=training, method="lda", trControl=fitControl)
model_svml			<- train(classe ~ ., data=training, method="svmLinear", trControl=fitControl)
model_svmr 			<- train(classe ~ ., data=training, method="svmRadial", trControl=fitControl)
model_LogitBoost 	<- train(classe ~ ., data=training, method="LogitBoost", trControl=fitControl)

## turn off parallel processing
stopCluster(cluster)
```

### Cross-Validation & Expected Out-Of-Sample Error
The next code chunk compared the performances of the various ML algorithms used by assembling and printing a table of the in-sample accuracies achieved by each of the ML algorithms after using the trained models to predict the $classe results for the training data sub-set and comparing them to the actual $classe variables using the {caret}confusionMatrix() routine. Recall that the error rate is simply 1 - accuracy.

```{r testing, cache=TRUE}
## Compare & print candidate model training accuracy results
output 				<- round(rbind(confusionMatrix(predict(model_rf, training), training$classe)$overall[1:4],
	  						confusionMatrix(predict(model_rpart, training), training$classe)$overall[1:4],
	  						confusionMatrix(predict(model_treebag, training), training$classe)$overall[1:4],
	  						confusionMatrix(predict(model_gbm, training), training$classe)$overall[1:4],
	  						confusionMatrix(predict(model_lda, training), training$classe)$overall[1:4],
	  						confusionMatrix(predict(model_svml, training), training$classe)$overall[1:4],
	  						confusionMatrix(predict(model_svmr, training), training$classe)$overall[1:4],
	  					confusionMatrix(predict(model_LogitBoost, training), training$classe)$overall[1:4]),5)
rownames(output) 	<- c("rf", "rpart", "treebag", "gbm", "lda", "svmLinear", "svmRadial", "LogitBoost")
kable(output)
```

From the results presented in the table above, it is clear that the random forest (rf) and bagged trees (treebag) algorithms gave the best results with in-sample accuracies at or near $100 \%$ (ie $0 \%$ in-sample error rate). The boosted trees (gbm) and support vector machines - radial (svmr) algorithms both gave in-sample accuracies in excess of $90 \%$ while all others fell in the range $50 - 90 \%$. Since the highest accuracy was achieved by the random forest algorithm (the same algorithm used in the original research paper), this method was selected for further testing at this point.

Initially the detailed in-sample test results for the random forest algorithm were printed out by the following code chunk using the {caret}confusionMatrix() routine. Then the trained random forest model was used to predict the $classe results for the testing sub-set of the partitioned data and details of the out-of-sample performance (cross-validation) were printed.  Finally a bar chart graphic comparing the in-sample and out-of-sample performances was prepared to allow easy comparison of the results.

```{r validation, cache=TRUE, fig.width=14, fig.height=8, fig.align='center', htmlcap='*Figure 1: Selected RF Model Performance*'}
## Print out details of selected model training performance
prediction 			<- predict(model_rf, training)
cm_training 		<- confusionMatrix(prediction, training$classe)
cm_training

## Use selected model to predict testing results and assess performance
prediction 			<- predict(model_rf, testing)
cm_testing 			<- confusionMatrix(prediction, testing$classe)
cm_testing

## Plot comparison charts for training and testing performance of selected model
par(mfrow=c(1,2))
plot(cm_training$table, col = cm_training$byClass, main = paste("RF Model In-Sample Accuracy =", 
																round(cm_training$overall['Accuracy'], 5)))
plot(cm_testing$table, col = cm_testing$byClass, main = paste("RF Model Out-Of-Sample Accuracy =", 
																round(cm_testing$overall['Accuracy'], 5)))
```

The detailed in-sample results for the random forest method show the very high accuracy performance achieved - $100 \%$. As expected, this performance was lower for the out-of-sample (manual cross-validation) testing but nevertheless exceeded $99.2 \%$ accuracy (or $< 0.8 \%$ out-of-sample error rate). At this accuracy level, the probability of achieving a perfect score for the $20$ blind test cases was computed as $0.9926^{20} \approx 0.8620$. But given that $2$ attempts were allowed, the probability that at least one of those attempts would achieve a perfect score was computed as $1-(1-0.8672)^{2} \approx 0.9809$, ie a $98.1 \%$ probability of achieving at least one perfect score in the two attempts allowed. On this basis, the decision was taken to proceed with the blind test case submission. Examining Figure 1 shows these very high levels of prediction accuracy graphically. On the the LHS the bar chart shows the perfect results achieved in-sample (hence the bars are shown filled in) compared to the RHS where the out-of-sample results do show the small (but acceptable) number of errors.

### Rationale For Design Choices & Submission
The text above describes the rationales for various design choices made, including:  
* Avoiding a time slice based approach  
* Filtering of the data sets  
* Partitioning of the data set to support manual cross-validation and out-of-sample performance estimation  
* Training of multiple ML algorithms  
* Optimisation of the computations   
* Selecting the ML algorithm for use with the blind test cases via in-sample and out-of-sample testing  
* Computing the likely performance against the blind test cases and justifying the decision to proceed.

But the ultimate rationale for these choices is the actual performance achieved in the grading of the $20$ blind test cases. The following code chunk was used to predict the (a-priori unknown) $classe results for each of the $20$ test cases and prepare the correctly formatted files for submission.

```{r submission, cache=TRUE}
## Use selected model to predict submission results
submission 			<- predict(model_rf, submissionCases)
output				<- rbind(as.character(submission[1:20]))
colnames(output) 	<- as.character(1:20)
## kable(output)

## Supplied function for creating results files
pml_write_files 	<- function(x){
  n 				<- length(x)
  for(i in 1:n){
    filename 		<- paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

## Create results files
pml_write_files(submission)
```

The files prepared above were then submitted to the grading process. A perfect score (20 out of 20 correct) was achieved on the first submission. This justifies the design choices made.

### Conclusions
The data sets required were loaded and pre-processed. A number of candidate ML algorithms were trained and assessed. The random forest method was selected and out-of-sample results assessed. Finally the blind test cases were computed and submitted, achieving a perfect score. These activities were described in the document above. Therefore, all the requirements for this project have been met.